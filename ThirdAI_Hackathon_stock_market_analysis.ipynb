{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zui1_i4MjVaD"
      },
      "source": [
        "## ThirdAI's NeuralDB\n",
        "\n",
        "NeuralDB, as the name suggests, is a combination of a neural network and a database. It provides a high-level API for users to insert different types of files into it and search through the file contents with natural language queries. The neural network part of it enables semantic search while the database part of it stores the paragraphs of the files that are inserted into it.\n",
        "\n",
        "First, let's install the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zAoAP1wjVaF",
        "outputId": "49ef01fd-ca46-4ebd-85fb-15d20ab91c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: thirdai in /usr/local/lib/python3.10/dist-packages (0.7.16)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from thirdai) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from thirdai) (4.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from thirdai) (2.27.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from thirdai) (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->thirdai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->thirdai) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai) (3.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas>=1.2.0->thirdai) (1.16.0)\n",
            "Requirement already satisfied: thirdai[neural_db] in /usr/local/lib/python3.10/dist-packages (0.7.16)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (1.22.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (4.7.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (2.27.1)\n",
            "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (1.5.3)\n",
            "Requirement already satisfied: PyTrie in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (0.4.0)\n",
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (1.22.5)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (0.0.252)\n",
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (0.0.1)\n",
            "Requirement already satisfied: trafilatura in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (1.2.2)\n",
            "Requirement already satisfied: python-docx in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (0.8.11)\n",
            "Requirement already satisfied: url-normalize in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (1.4.3)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (3.8.1)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (1.3.6)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (from thirdai[neural_db]) (1.10.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->thirdai[neural_db]) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.2.0->thirdai[neural_db]) (2022.7.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from bs4->thirdai[neural_db]) (4.11.2)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (0.0.19)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (2.8.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (1.2.4)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->thirdai[neural_db]) (8.2.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai[neural_db]) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai[neural_db]) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai[neural_db]) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->thirdai[neural_db]) (3.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->thirdai[neural_db]) (8.1.6)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->thirdai[neural_db]) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->thirdai[neural_db]) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->thirdai[neural_db]) (4.65.0)\n",
            "Requirement already satisfied: lxml>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from python-docx->thirdai[neural_db]) (4.9.3)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.10/dist-packages (from PyTrie->thirdai[neural_db]) (2.4.0)\n",
            "Requirement already satisfied: courlan>=0.7.2 in /usr/local/lib/python3.10/dist-packages (from trafilatura->thirdai[neural_db]) (0.9.3)\n",
            "Requirement already satisfied: htmldate>=1.2.1 in /usr/local/lib/python3.10/dist-packages (from trafilatura->thirdai[neural_db]) (1.2.3)\n",
            "Requirement already satisfied: justext>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from trafilatura->thirdai[neural_db]) (3.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from url-normalize->thirdai[neural_db]) (1.16.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai[neural_db]) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai[neural_db]) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai[neural_db]) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai[neural_db]) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->thirdai[neural_db]) (1.3.1)\n",
            "Requirement already satisfied: langcodes>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from courlan>=0.7.2->trafilatura->thirdai[neural_db]) (3.3.0)\n",
            "Requirement already satisfied: tld>=0.13 in /usr/local/lib/python3.10/dist-packages (from courlan>=0.7.2->trafilatura->thirdai[neural_db]) (0.13)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->thirdai[neural_db]) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain->thirdai[neural_db]) (0.9.0)\n",
            "Requirement already satisfied: dateparser>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from htmldate>=1.2.1->trafilatura->thirdai[neural_db]) (1.1.8)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->thirdai[neural_db]) (2.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->bs4->thirdai[neural_db]) (2.4.1)\n",
            "Requirement already satisfied: tzlocal in /usr/local/lib/python3.10/dist-packages (from dateparser>=1.1.1->htmldate>=1.2.1->trafilatura->thirdai[neural_db]) (5.0.1)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain->thirdai[neural_db]) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain->thirdai[neural_db]) (1.0.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.0.252)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.0.19)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.22.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (1.0.0)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.10/dist-packages (0.27.8)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai) (3.8.5)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (1.3.1)\n",
            "Requirement already satisfied: paper-qa in /usr/local/lib/python3.10/dist-packages (3.5.0)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.10/dist-packages (from paper-qa) (3.14.0)\n",
            "Requirement already satisfied: langchain>=0.0.198 in /usr/local/lib/python3.10/dist-packages (from paper-qa) (0.0.252)\n",
            "Requirement already satisfied: openai>=0.27.8 in /usr/local/lib/python3.10/dist-packages (from paper-qa) (0.27.8)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (from paper-qa) (1.7.4)\n",
            "Requirement already satisfied: PyCryptodome in /usr/local/lib/python3.10/dist-packages (from paper-qa) (3.18.0)\n",
            "Requirement already satisfied: html2text in /usr/local/lib/python3.10/dist-packages (from paper-qa) (2020.1.16)\n",
            "Requirement already satisfied: tiktoken>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from paper-qa) (0.4.0)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (2.0.19)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (3.8.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (4.0.2)\n",
            "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (0.5.14)\n",
            "Requirement already satisfied: langsmith<0.1.0,>=0.0.11 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (0.0.19)\n",
            "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (2.8.4)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (1.22.4)\n",
            "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (1.2.4)\n",
            "Requirement already satisfied: pydantic<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (1.10.12)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (2.27.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain>=0.0.198->paper-qa) (8.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai>=0.27.8->paper-qa) (4.65.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken>=0.4.0->paper-qa) (2022.10.31)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain>=0.0.198->paper-qa) (1.3.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (3.20.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2,>=1->langchain>=0.0.198->paper-qa) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (2023.7.22)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain>=0.0.198->paper-qa) (3.4)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain>=0.0.198->paper-qa) (2.0.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.10/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (23.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain>=0.0.198->paper-qa) (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip3 install thirdai --upgrade\n",
        "!pip3 install thirdai[neural_db]\n",
        "!pip3 install langchain --upgrade\n",
        "!pip3 install openai --upgrade\n",
        "!pip3 install paper-qa --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88FjJtvCjVaF"
      },
      "outputs": [],
      "source": [
        "from thirdai import licensing, neural_db as ndb\n",
        "licensing.deactivate()\n",
        "licensing.activate(\"1FB7DD-CAC3EC-832A67-84208D-C4E39E-V3\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2oh7JXJjVaG"
      },
      "source": [
        "Now, let's import the relevant module and define a neural db class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ez7C5yAKjVaG"
      },
      "outputs": [],
      "source": [
        "db = ndb.NeuralDB(user_id=\"my_user\") # you can use any username, in the future, this username will let you push models to the model hub"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtC3eAIkjVaG"
      },
      "source": [
        "### You even load from a base DB from our Bazaar (optional but recommended)\n",
        "\n",
        "We have a model bazaar that provides users with domain specific NeuralDBs that can jumpstart searching on their private documents. The Bazaar has two main types of DBs\n",
        "\n",
        "1. Base DBs: These come with models that have either general QnA capabilities or domain specific capabilities like search on Medical Documents, Financial documents or Contracts. These come with an empty data index into which users can insert their files.\n",
        "\n",
        "2. Pre-Indexed DBs: These are ready-to-search DBs that come with pre-trained models and their corresponding datasets. These are meant to  search through large public datasets like PubMed or Amazon 3MM Products or Stackoverflow issues etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cjSBCgxjjVaG"
      },
      "outputs": [],
      "source": [
        "# Set up a cache directory\n",
        "import os\n",
        "if not os.path.isdir(\"bazaar_cache\"):\n",
        "    os.mkdir(\"bazaar_cache\")\n",
        "\n",
        "from pathlib import Path\n",
        "from thirdai.neural_db import Bazaar\n",
        "bazaar = Bazaar(cache_dir=Path(\"bazaar_cache\"))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMBhmBfDjVaH"
      },
      "source": [
        "Call fetch to refresh list of available DBs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "fYVb_JmFjVaH"
      },
      "outputs": [],
      "source": [
        "bazaar.fetch() # Optional arg filter=\"model name\" to filter by model name.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wLYW2kfjVaH"
      },
      "source": [
        "Below is the list of all DBs in the Bazaar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBK5CQGwjVaH",
        "outputId": "f7d51b0e-d5b4-4f1f-fbde-9611237d7e44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Contract Review', 'General QnA', 'Finance QnA']\n"
          ]
        }
      ],
      "source": [
        "print(bazaar.list_model_names())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmjwqeC0jVaI"
      },
      "source": [
        "Finally load the DB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "9NLtXNyEjVaI"
      },
      "outputs": [],
      "source": [
        "db = bazaar.get_model(\"General QnA\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LK2qt9SnjVaI"
      },
      "source": [
        "### Insert your files\n",
        "\n",
        "Let's insert things into it!\n",
        "\n",
        "Currently, we natively support adding CSV, PDF and DOCX files. We also have a support to automatically scrape and parse URLs. All other file formats have to be converted into CSV files where each row represents a paragraph/text-chunk of the document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7aQwTZjjVaI"
      },
      "source": [
        "#### Example 1: CSV files\n",
        "The first example below shows how to insert a CSV file. Please note that a CSV file is required to have a column named \"DOC_ID\" with rows numbered from 0 to n_rows-1."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cQE0bEol8bs",
        "outputId": "6f1b68bb-8ff2-44bd-f392-360f9d9a0aa4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "NcLYFGoHjVaI"
      },
      "outputs": [],
      "source": [
        "insertable_docs = []\n",
        "csv_files = ['Stocks_Dataset.csv']\n",
        "\n",
        "for file in csv_files:\n",
        "    csv_doc = ndb.CSV(\n",
        "        path=file,\n",
        "        id_column=\"DOC_ID\",\n",
        "        strong_columns=[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"Name\"],\n",
        "        weak_columns=[\"high\", \"low\"],\n",
        "        reference_columns=[\"date\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"Name\"])\n",
        "    #\n",
        "    insertable_docs.append(csv_doc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wUCyzI8ajVaI"
      },
      "source": [
        "#### Example 2: PDF files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "SJFilNwhjVaI"
      },
      "outputs": [],
      "source": [
        "insertable_docs = []\n",
        "pdf_files = ['analysis.pdf']\n",
        "\n",
        "for file in pdf_files:\n",
        "    pdf_doc = ndb.PDF(file)\n",
        "    insertable_docs.append(pdf_doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iWkemY4WjVaJ"
      },
      "source": [
        "### Insert into NeuralDB\n",
        "\n",
        "If you wish to insert without unsupervised training, you can set 'train=False' in the insert() method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Y51pFUB3jVaJ"
      },
      "outputs": [],
      "source": [
        "source_ids = db.insert(insertable_docs, train=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3I2nkfhjVaJ"
      },
      "source": [
        "The above command is intended to be used with a base DB which already has reasonable knowledge of the domain. In general, we always recommend using 'train=True' as shown below.\n",
        "\n",
        "#### Insert and Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9e0Eu21jVaJ",
        "outputId": "fc14dace-34bd-4a91-a4ac-2b789a2b40ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded data | source 'Documents:\n",
            "analysis.pdf' | vectors 1326 | batches 1 | time 0s | complete\n",
            "\n",
            "train | epoch 0 | train_steps 2467 | train_hash_precision@5=0.384465  | train_batches 1 | time 7s\n",
            "\n",
            "train | epoch 1 | train_steps 2468 | train_hash_precision@5=0.773152  | train_batches 1 | time 4s\n",
            "\n",
            "train | epoch 2 | train_steps 2469 | train_hash_precision@5=0.935445  | train_batches 1 | time 13s\n",
            "\n",
            "train | epoch 3 | train_steps 2470 | train_hash_precision@5=0.966817  | train_batches 1 | time 8s\n",
            "\n",
            "train | epoch 4 | train_steps 2471 | train_hash_precision@5=0.982956  | train_batches 1 | time 9s\n",
            "\n",
            "train | epoch 5 | train_steps 2472 | train_hash_precision@5=0.99095  | train_batches 1 | time 4s\n",
            "\n"
          ]
        }
      ],
      "source": [
        "source_ids = db.insert(insertable_docs, train=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tcN00b1jVaJ"
      },
      "source": [
        "If you call the insert() method multiple times, the documents will automatically be de-duplicated. If insert=True, then the training will be done multiple times."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12S7EGyfjVaJ"
      },
      "source": [
        "### Search\n",
        "\n",
        "Now let's start searching."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFrzHytJjVaJ",
        "outputId": "7e55a8d0-a0d1-4480-f1ba-9d8c64f0768f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Gold to Dow ratio reached an all-time-high of 1.01 in January 1980 when the price of gold hit $878 and the Dow Jones was trading in a range. Since stocks outperformed gold almost uninterruptedly for 2 decades until August 1999 when the ratio reached an all time low of 0.02.\n",
            "************\n",
            "According to Algorithm 1 we should build the forecast model for the unconstrained four- dimensional time series data {Yt}T t=1. Without loss of generality we first assume that all time series in Yt are stationary then a p-order (p  1) VAR model denoted by VAR(p) can be formulated as: Yt = a + A1Yt-1 + ... + ApYt-p + wt = a + p  j=1 AjYt-j + wt t = (p + 1) ... T (11) where Yt-j is the j-th lag of Yt; a = (a1 a2 a3 a4)T is a four-dementional vector of intercepts; Aj stands for the time-invariant 4 x 4 coefficient matrix; and wt = (w(1) t w(2) t w(3) t w(4) t )T is a four-dementional error term vector satisfying: (1) Mean zero: E(wt) = 0; (2) No correlation across time: E(wT t-kwt) = 0 for any non-zero k. Writing Eq.\n",
            "************\n"
          ]
        }
      ],
      "source": [
        "search_results = db.search(\n",
        "    query=\"what was in the dataset?\",\n",
        "    top_k=2,\n",
        "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
        "\n",
        "for result in search_results:\n",
        "    print(result.text)\n",
        "    # print(result.context(radius=1))\n",
        "    # print(result.source)\n",
        "    # print(result.metadata)\n",
        "    print('************')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxWSuMFejVaJ"
      },
      "source": [
        "We can see that the search pulled up the right passage that contains the termination period \"(i) five (5) years or (ii) when the confidential information no longer qualifies as a trade secret\" ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "y8a5ltFfjVaK",
        "outputId": "d332878c-5c25-4952-99df-e6105bb4cd64",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If limit-up(limit-down) happens we firstly multiply x(c)(x(o)) and x(h) by 1.1 to make a relatively large interval. And then conduct measurements given in circumstances (2) and (3). In summary the general forecasting framework for OHLC data with T periods is described in Algorithm 1.\n",
            "************\n",
            "Algorithm 1 General forecasting framework for OHLC data 1: Get the raw candlestick charts with T periods from the capital market; 2: Extract the four-dimensional time series data of the candlestick charts record as {Xt}T t=1; 3: Conduct transformation method to {Xt}T t=1 and obtain {Yt}T t=1 according to Eq.\n",
            "************\n"
          ]
        }
      ],
      "source": [
        "search_results = db.search(\n",
        "    query=\"General forecasting framework for OHLC data\",\n",
        "    top_k=2,\n",
        "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
        "\n",
        "for result in search_results:\n",
        "    print(result.text)\n",
        "    # print(result.context(radius=1))\n",
        "    # print(result.source)\n",
        "    # print(result.metadata)\n",
        "    print('************')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TpZBT47ojVaK"
      },
      "source": [
        "We can see that the search pulled up the right passage again that has \"made by and between\".\n",
        "\n",
        "Now let's ask a tricky question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "TSezw5oFjVaK",
        "outputId": "97de3187-333b-44f1-cd40-380557efbbe6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A trade-off must be evaluated to choose p the common used criterions in practice are AIC BIC and HQ (Hannan-Quinn). In this paper we prefer AIC because of its conciseness which is formulated as AIC(p) = ln 4 i=1 T j=1 ^u2 ij T + 2pK2 T (15) 14 where T stands for the total period number of OHLC series p is VAR lag order K is the VAR dimension and ^uij = ^Y (i) j - Y (i) j (1 <= i <= 4 1 <= j <= T) represents for the residuals of the VAR model.\n",
            "************\n",
            "Finally the simulated OHLC data {Xt}T t=1 are generated by applying the inverse transformation formula in Eq. (9). In order to evaluate the performance of the proposed method with different variance com- ponent levels we consider the following scenarios: Scenario 1: p = 1 T = 220 Y1 = [4 0.7 -0.85 0]T and A1 =             0.55 0.12 0.12 0.12 0.12 0.55 0.12 0.12 0.12 0.12 0.55 0.12 0.12 0.12 0.12 0.55             and Sw is a 4 x 4 diagonal matrix with diagonal element being 0.052 i.e. Sw = diag{0.052 0.052 0.052 0.052}.\n",
            "************\n"
          ]
        }
      ],
      "source": [
        "search_results = db.search(\n",
        "    query=\"AIC is formulated as\",\n",
        "    top_k=2,\n",
        "    on_error=lambda error_msg: print(f\"Error! {error_msg}\"))\n",
        "\n",
        "for result in search_results:\n",
        "    print(result.text)\n",
        "    # print(result.context(radius=1))\n",
        "    # print(result.source)\n",
        "    # print(result.metadata)\n",
        "    print('************')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZpQcs4CjVaZ"
      },
      "source": [
        "### Get Answers from OpenAI using Langchain\n",
        "\n",
        "In this section, we will show how to use LangChain and query OpenAI's QnA module to generate an answer from the references that you retrieve from the above DB. You'll have to specify your own OpenAI key for this module to work. You can replace this segment with any other generative model of your choice. You can choose to use an source model like MPT or Dolly for answer generation with the same prompt that you use with OpenAI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "7xM3LiMPjVaZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "6cgAxYF5jVaZ"
      },
      "outputs": [],
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from paperqa.prompts import qa_prompt\n",
        "from paperqa.chains import make_chain\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model_name='gpt-3.5-turbo',\n",
        "    temperature=0.1,\n",
        ")\n",
        "\n",
        "qa_chain = make_chain(prompt=qa_prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "0cg1UqS1jVaZ"
      },
      "outputs": [],
      "source": [
        "def get_references(query):\n",
        "    search_results = db.search(query,top_k=3)\n",
        "    references = []\n",
        "    for result in search_results:\n",
        "        references.append(result.text)\n",
        "    return references\n",
        "\n",
        "def get_answer(query, references):\n",
        "    return qa_chain.run(question=query, context='\\n\\n'.join(references[:3]), answer_length=\"abt 50 words\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "sc0nTgawjVaa",
        "outputId": "106df568-75ba-4e86-f23a-86f25ec30175",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A trade-off must be evaluated to choose p the common used criterions in practice are AIC BIC and HQ (Hannan-Quinn). In this paper we prefer AIC because of its conciseness which is formulated as AIC(p) = ln 4 i=1 T j=1 ^u2 ij T + 2pK2 T (15) 14 where T stands for the total period number of OHLC series p is VAR lag order K is the VAR dimension and ^uij = ^Y (i) j - Y (i) j (1 <= i <= 4 1 <= j <= T) represents for the residuals of the VAR model.', 'Finally the simulated OHLC data {Xt}T t=1 are generated by applying the inverse transformation formula in Eq. (9). In order to evaluate the performance of the proposed method with different variance com- ponent levels we consider the following scenarios: Scenario 1: p = 1 T = 220 Y1 = [4 0.7 -0.85 0]T and A1 =             0.55 0.12 0.12 0.12 0.12 0.55 0.12 0.12 0.12 0.12 0.55 0.12 0.12 0.12 0.12 0.55             and Sw is a 4 x 4 diagonal matrix with diagonal element being 0.052 i.e. Sw = diag{0.052 0.052 0.052 0.052}.', 'Special offer: get a $15 bonus on any TradingView plan when you sign-up through this link. The bonus will be granted in the form of \"coins\" that you can apply towards future purchases.']\n"
          ]
        }
      ],
      "source": [
        "query = \"AIC is formulated as\"\n",
        "\n",
        "references = get_references(query)\n",
        "print(references)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "lF9V-B9kjVaa",
        "outputId": "fe0e0a35-cabe-406c-c474-fdd485a2e9ec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AIC is formulated as AIC(p) = ln 4 i=1 T j=1 ^u2 ij T + 2pK2 T (15) 14, where p is the VAR lag order, T is the total period number of OHLC series, K is the VAR dimension, and ^uij = ^Y (i) j - Y (i) j represents the residuals of the VAR model (1 <= i <= 4, 1 <= j <= T) (Example2012).\n"
          ]
        }
      ],
      "source": [
        "answer = get_answer(query, references)\n",
        "\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRXeHXmYjVaa"
      },
      "source": [
        "### Load and Save\n",
        "As usual, saving and loading the DB are one-liners."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "7nnxo86BjVaa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eaf908ae-4865-4c2a-a42e-41acd2b2b8a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.16666666666666666% done with loading.\n",
            "0.3333333333333333% done with loading.\n",
            "0.5% done with loading.\n",
            "0.6666666666666666% done with loading.\n",
            "0.8333333333333334% done with loading.\n",
            "1.0% done with loading.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<thirdai.neural_db.neural_db.NeuralDB at 0x7efce4eb9690>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "# save your db\n",
        "db.save(\"data.db\")\n",
        "\n",
        "# Loading is just like we showed above, with an optional progress handler\n",
        "db.from_checkpoint(\"data.db\", on_progress=lambda fraction: print(f\"{fraction}% done with loading.\"))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}